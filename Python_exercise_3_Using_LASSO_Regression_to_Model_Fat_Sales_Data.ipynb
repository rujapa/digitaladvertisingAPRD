{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python exercise 3 - Using LASSO Regression to Model Fat Sales Data",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rujapa/digitaladvertisingAPRD/blob/master/Python_exercise_3_Using_LASSO_Regression_to_Model_Fat_Sales_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UuQ3h3JRYf5",
        "colab_type": "text"
      },
      "source": [
        "Python Exercise 3: Using LASSO Regression to Model Fat Sales Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1VaOOPcaGEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO_4ATVuaWE2",
        "colab_type": "code",
        "outputId": "49059f1d-ae45-45c6-976f-6ddcec000803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#import csv file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "alldata=pd.read_csv(open('drive/My Drive/APRD Digital ads/Homework/finalmaster-ratios.csv','rb'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8xXbkY3bKHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LassoLarsCV\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGOnyzXHfQ9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a list of all the predictors that we want to feed in the LassoLarsCV model\n",
        "allvariablenames = list(alldata.columns.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQAIPiC2bEPF",
        "colab_type": "code",
        "outputId": "92931354-7baf-40ec-af78-f0ca95996e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "allvariablenames"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['# Purchases',\n",
              " 'B01001001',\n",
              " 'B01001002',\n",
              " 'B01001003',\n",
              " 'B01001004',\n",
              " 'B01001005',\n",
              " 'B01001006',\n",
              " 'B01001007',\n",
              " 'B01001008',\n",
              " 'B01001009',\n",
              " 'B01001010',\n",
              " 'B01001011',\n",
              " 'B01001012',\n",
              " 'B01001013',\n",
              " 'B01001014',\n",
              " 'B01001015',\n",
              " 'B01001016',\n",
              " 'B01001017',\n",
              " 'B01001018',\n",
              " 'B01001019',\n",
              " 'B01001020',\n",
              " 'B01001021',\n",
              " 'B01001022',\n",
              " 'B01001023',\n",
              " 'B01001024',\n",
              " 'B01001025',\n",
              " 'B01001026',\n",
              " 'B01001027',\n",
              " 'B01001028',\n",
              " 'B01001029',\n",
              " 'B01001030',\n",
              " 'B01001031',\n",
              " 'B01001032',\n",
              " 'B01001033',\n",
              " 'B01001034',\n",
              " 'B01001035',\n",
              " 'B01001036',\n",
              " 'B01001037',\n",
              " 'B01001038',\n",
              " 'B01001039',\n",
              " 'B01001040',\n",
              " 'B01001041',\n",
              " 'B01001042',\n",
              " 'B01001043',\n",
              " 'B01001044',\n",
              " 'B01001045',\n",
              " 'B01001046',\n",
              " 'B01001047',\n",
              " 'B01001048',\n",
              " 'B01001049',\n",
              " 'B02001001',\n",
              " 'B02001002',\n",
              " 'B02001003',\n",
              " 'B02001004',\n",
              " 'B02001005',\n",
              " 'B02001006',\n",
              " 'B02001007',\n",
              " 'B02001008',\n",
              " 'B02001009',\n",
              " 'B02001010',\n",
              " 'B12001001',\n",
              " 'B12001002',\n",
              " 'B12001003',\n",
              " 'B12001004',\n",
              " 'B12001005',\n",
              " 'B12001006',\n",
              " 'B12001007',\n",
              " 'B12001008',\n",
              " 'B12001009',\n",
              " 'B12001010',\n",
              " 'B12001011',\n",
              " 'B12001012',\n",
              " 'B12001013',\n",
              " 'B12001014',\n",
              " 'B12001015',\n",
              " 'B12001016',\n",
              " 'B12001017',\n",
              " 'B12001018',\n",
              " 'B12001019',\n",
              " 'B13014001',\n",
              " 'B13014002',\n",
              " 'B13014003',\n",
              " 'B13014004',\n",
              " 'B13014005',\n",
              " 'B13014006',\n",
              " 'B13014007',\n",
              " 'B13014008',\n",
              " 'B13014009',\n",
              " 'B13014010',\n",
              " 'B13014011',\n",
              " 'B13014012',\n",
              " 'B13014013',\n",
              " 'B13014014',\n",
              " 'B13014015',\n",
              " 'B13014016',\n",
              " 'B13014017',\n",
              " 'B13014018',\n",
              " 'B13014019',\n",
              " 'B13014020',\n",
              " 'B13014021',\n",
              " 'B13014022',\n",
              " 'B13014023',\n",
              " 'B13014024',\n",
              " 'B13014025',\n",
              " 'B13014026',\n",
              " 'B13014027',\n",
              " 'B13015001',\n",
              " 'B13015002',\n",
              " 'B13015003',\n",
              " 'B13015004',\n",
              " 'B13015005',\n",
              " 'B13015006',\n",
              " 'B13015007',\n",
              " 'B13015008',\n",
              " 'B13015009',\n",
              " 'B13015010',\n",
              " 'B13015011',\n",
              " 'B13015012',\n",
              " 'B13015013',\n",
              " 'B13015014',\n",
              " 'B13015015',\n",
              " 'B13016001',\n",
              " 'B13016002',\n",
              " 'B13016003',\n",
              " 'B13016004',\n",
              " 'B13016005',\n",
              " 'B13016006',\n",
              " 'B13016007',\n",
              " 'B13016008',\n",
              " 'B13016009',\n",
              " 'B13016010',\n",
              " 'B13016011',\n",
              " 'B13016012',\n",
              " 'B13016013',\n",
              " 'B13016014',\n",
              " 'B13016015',\n",
              " 'B13016016',\n",
              " 'B13016017',\n",
              " 'B15002001',\n",
              " 'B15002002',\n",
              " 'B15002003',\n",
              " 'B15002004',\n",
              " 'B15002005',\n",
              " 'B15002006',\n",
              " 'B15002007',\n",
              " 'B15002008',\n",
              " 'B15002009',\n",
              " 'B15002010',\n",
              " 'B15002011',\n",
              " 'B15002012',\n",
              " 'B15002013',\n",
              " 'B15002014',\n",
              " 'B15002015',\n",
              " 'B15002016',\n",
              " 'B15002017',\n",
              " 'B15002018',\n",
              " 'B15002019',\n",
              " 'B15002020',\n",
              " 'B15002021',\n",
              " 'B15002022',\n",
              " 'B15002023',\n",
              " 'B15002024',\n",
              " 'B15002025',\n",
              " 'B15002026',\n",
              " 'B15002027',\n",
              " 'B15002028',\n",
              " 'B15002029',\n",
              " 'B15002030',\n",
              " 'B15002031',\n",
              " 'B15002032',\n",
              " 'B15002033',\n",
              " 'B15002034',\n",
              " 'B15002035',\n",
              " 'B19001001',\n",
              " 'B19001002',\n",
              " 'B19001003',\n",
              " 'B19001004',\n",
              " 'B19001005',\n",
              " 'B19001006',\n",
              " 'B19001007',\n",
              " 'B19001008',\n",
              " 'B19001009',\n",
              " 'B19001010',\n",
              " 'B19001011',\n",
              " 'B19001012',\n",
              " 'B19001013',\n",
              " 'B19001014',\n",
              " 'B19001015',\n",
              " 'B19001016',\n",
              " 'B19001017']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjrmr5goa5O6",
        "colab_type": "code",
        "outputId": "eb62cf92-bb83-46c7-be4f-0ac7e35a36cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "alldata"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th># Purchases</th>\n",
              "      <th>B01001001</th>\n",
              "      <th>B01001002</th>\n",
              "      <th>B01001003</th>\n",
              "      <th>B01001004</th>\n",
              "      <th>B01001005</th>\n",
              "      <th>B01001006</th>\n",
              "      <th>B01001007</th>\n",
              "      <th>B01001008</th>\n",
              "      <th>B01001009</th>\n",
              "      <th>B01001010</th>\n",
              "      <th>B01001011</th>\n",
              "      <th>B01001012</th>\n",
              "      <th>B01001013</th>\n",
              "      <th>B01001014</th>\n",
              "      <th>B01001015</th>\n",
              "      <th>B01001016</th>\n",
              "      <th>B01001017</th>\n",
              "      <th>B01001018</th>\n",
              "      <th>B01001019</th>\n",
              "      <th>B01001020</th>\n",
              "      <th>B01001021</th>\n",
              "      <th>B01001022</th>\n",
              "      <th>B01001023</th>\n",
              "      <th>B01001024</th>\n",
              "      <th>B01001025</th>\n",
              "      <th>B01001026</th>\n",
              "      <th>B01001027</th>\n",
              "      <th>B01001028</th>\n",
              "      <th>B01001029</th>\n",
              "      <th>B01001030</th>\n",
              "      <th>B01001031</th>\n",
              "      <th>B01001032</th>\n",
              "      <th>B01001033</th>\n",
              "      <th>B01001034</th>\n",
              "      <th>B01001035</th>\n",
              "      <th>B01001036</th>\n",
              "      <th>B01001037</th>\n",
              "      <th>B01001038</th>\n",
              "      <th>B01001039</th>\n",
              "      <th>...</th>\n",
              "      <th>B15002013</th>\n",
              "      <th>B15002014</th>\n",
              "      <th>B15002015</th>\n",
              "      <th>B15002016</th>\n",
              "      <th>B15002017</th>\n",
              "      <th>B15002018</th>\n",
              "      <th>B15002019</th>\n",
              "      <th>B15002020</th>\n",
              "      <th>B15002021</th>\n",
              "      <th>B15002022</th>\n",
              "      <th>B15002023</th>\n",
              "      <th>B15002024</th>\n",
              "      <th>B15002025</th>\n",
              "      <th>B15002026</th>\n",
              "      <th>B15002027</th>\n",
              "      <th>B15002028</th>\n",
              "      <th>B15002029</th>\n",
              "      <th>B15002030</th>\n",
              "      <th>B15002031</th>\n",
              "      <th>B15002032</th>\n",
              "      <th>B15002033</th>\n",
              "      <th>B15002034</th>\n",
              "      <th>B15002035</th>\n",
              "      <th>B19001001</th>\n",
              "      <th>B19001002</th>\n",
              "      <th>B19001003</th>\n",
              "      <th>B19001004</th>\n",
              "      <th>B19001005</th>\n",
              "      <th>B19001006</th>\n",
              "      <th>B19001007</th>\n",
              "      <th>B19001008</th>\n",
              "      <th>B19001009</th>\n",
              "      <th>B19001010</th>\n",
              "      <th>B19001011</th>\n",
              "      <th>B19001012</th>\n",
              "      <th>B19001013</th>\n",
              "      <th>B19001014</th>\n",
              "      <th>B19001015</th>\n",
              "      <th>B19001016</th>\n",
              "      <th>B19001017</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>206252</td>\n",
              "      <td>469.226965</td>\n",
              "      <td>31.432422</td>\n",
              "      <td>35.219052</td>\n",
              "      <td>33.628765</td>\n",
              "      <td>20.121017</td>\n",
              "      <td>12.610787</td>\n",
              "      <td>6.734480</td>\n",
              "      <td>6.225394</td>\n",
              "      <td>19.432539</td>\n",
              "      <td>28.101546</td>\n",
              "      <td>28.421543</td>\n",
              "      <td>26.390047</td>\n",
              "      <td>31.989993</td>\n",
              "      <td>31.359696</td>\n",
              "      <td>32.116052</td>\n",
              "      <td>32.213021</td>\n",
              "      <td>12.184124</td>\n",
              "      <td>18.361034</td>\n",
              "      <td>9.454454</td>\n",
              "      <td>15.175610</td>\n",
              "      <td>16.281054</td>\n",
              "      <td>11.025348</td>\n",
              "      <td>6.230243</td>\n",
              "      <td>4.518744</td>\n",
              "      <td>530.773035</td>\n",
              "      <td>31.999690</td>\n",
              "      <td>34.322091</td>\n",
              "      <td>32.649380</td>\n",
              "      <td>20.101623</td>\n",
              "      <td>12.513818</td>\n",
              "      <td>8.072649</td>\n",
              "      <td>6.021760</td>\n",
              "      <td>22.923414</td>\n",
              "      <td>31.335454</td>\n",
              "      <td>31.558482</td>\n",
              "      <td>31.063941</td>\n",
              "      <td>36.082074</td>\n",
              "      <td>34.845723</td>\n",
              "      <td>...</td>\n",
              "      <td>64.610300</td>\n",
              "      <td>31.449746</td>\n",
              "      <td>58.735313</td>\n",
              "      <td>20.071053</td>\n",
              "      <td>6.726751</td>\n",
              "      <td>5.882267</td>\n",
              "      <td>543.803963</td>\n",
              "      <td>6.974272</td>\n",
              "      <td>2.504332</td>\n",
              "      <td>5.904107</td>\n",
              "      <td>11.917415</td>\n",
              "      <td>10.767170</td>\n",
              "      <td>18.141844</td>\n",
              "      <td>19.779852</td>\n",
              "      <td>10.956451</td>\n",
              "      <td>181.418442</td>\n",
              "      <td>26.717724</td>\n",
              "      <td>85.271036</td>\n",
              "      <td>54.243532</td>\n",
              "      <td>72.647457</td>\n",
              "      <td>30.816383</td>\n",
              "      <td>2.831933</td>\n",
              "      <td>2.912014</td>\n",
              "      <td>1000</td>\n",
              "      <td>105.667996</td>\n",
              "      <td>82.298375</td>\n",
              "      <td>68.141163</td>\n",
              "      <td>67.336195</td>\n",
              "      <td>63.566902</td>\n",
              "      <td>59.439845</td>\n",
              "      <td>49.409690</td>\n",
              "      <td>53.306757</td>\n",
              "      <td>42.318307</td>\n",
              "      <td>83.167229</td>\n",
              "      <td>89.249208</td>\n",
              "      <td>102.141470</td>\n",
              "      <td>52.872330</td>\n",
              "      <td>36.440765</td>\n",
              "      <td>23.446284</td>\n",
              "      <td>21.197485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>61399</td>\n",
              "      <td>486.538869</td>\n",
              "      <td>22.899396</td>\n",
              "      <td>21.531295</td>\n",
              "      <td>27.036271</td>\n",
              "      <td>16.808091</td>\n",
              "      <td>28.355511</td>\n",
              "      <td>18.192479</td>\n",
              "      <td>13.534422</td>\n",
              "      <td>21.466148</td>\n",
              "      <td>24.886399</td>\n",
              "      <td>23.534585</td>\n",
              "      <td>21.319565</td>\n",
              "      <td>27.101419</td>\n",
              "      <td>30.961416</td>\n",
              "      <td>37.117868</td>\n",
              "      <td>36.466392</td>\n",
              "      <td>12.557208</td>\n",
              "      <td>20.554081</td>\n",
              "      <td>12.182609</td>\n",
              "      <td>15.651721</td>\n",
              "      <td>20.668089</td>\n",
              "      <td>15.961172</td>\n",
              "      <td>10.423623</td>\n",
              "      <td>7.329110</td>\n",
              "      <td>513.461131</td>\n",
              "      <td>18.974250</td>\n",
              "      <td>23.404290</td>\n",
              "      <td>23.892897</td>\n",
              "      <td>17.036108</td>\n",
              "      <td>35.310021</td>\n",
              "      <td>18.534504</td>\n",
              "      <td>17.101256</td>\n",
              "      <td>22.785387</td>\n",
              "      <td>22.150198</td>\n",
              "      <td>22.622518</td>\n",
              "      <td>21.303279</td>\n",
              "      <td>26.971123</td>\n",
              "      <td>32.329517</td>\n",
              "      <td>...</td>\n",
              "      <td>56.929829</td>\n",
              "      <td>46.381727</td>\n",
              "      <td>65.707446</td>\n",
              "      <td>35.509451</td>\n",
              "      <td>16.782205</td>\n",
              "      <td>9.201536</td>\n",
              "      <td>515.086529</td>\n",
              "      <td>3.017306</td>\n",
              "      <td>1.047329</td>\n",
              "      <td>1.371503</td>\n",
              "      <td>6.358785</td>\n",
              "      <td>4.937410</td>\n",
              "      <td>8.303825</td>\n",
              "      <td>9.700264</td>\n",
              "      <td>7.555733</td>\n",
              "      <td>174.155902</td>\n",
              "      <td>25.834123</td>\n",
              "      <td>60.146626</td>\n",
              "      <td>62.440776</td>\n",
              "      <td>76.604658</td>\n",
              "      <td>55.383771</td>\n",
              "      <td>8.977108</td>\n",
              "      <td>9.251409</td>\n",
              "      <td>1000</td>\n",
              "      <td>71.289558</td>\n",
              "      <td>59.062447</td>\n",
              "      <td>54.704688</td>\n",
              "      <td>60.966323</td>\n",
              "      <td>53.012354</td>\n",
              "      <td>60.881706</td>\n",
              "      <td>59.231680</td>\n",
              "      <td>50.093078</td>\n",
              "      <td>40.700626</td>\n",
              "      <td>92.612963</td>\n",
              "      <td>117.363344</td>\n",
              "      <td>113.344051</td>\n",
              "      <td>75.774243</td>\n",
              "      <td>33.000508</td>\n",
              "      <td>33.169741</td>\n",
              "      <td>24.792689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>73170</td>\n",
              "      <td>489.859232</td>\n",
              "      <td>28.905289</td>\n",
              "      <td>36.271696</td>\n",
              "      <td>28.235616</td>\n",
              "      <td>21.566216</td>\n",
              "      <td>12.218122</td>\n",
              "      <td>7.243406</td>\n",
              "      <td>7.380074</td>\n",
              "      <td>16.933169</td>\n",
              "      <td>24.914582</td>\n",
              "      <td>26.896269</td>\n",
              "      <td>31.802651</td>\n",
              "      <td>30.531639</td>\n",
              "      <td>36.258029</td>\n",
              "      <td>35.998360</td>\n",
              "      <td>33.429001</td>\n",
              "      <td>13.625803</td>\n",
              "      <td>19.406861</td>\n",
              "      <td>12.245456</td>\n",
              "      <td>14.664480</td>\n",
              "      <td>21.169878</td>\n",
              "      <td>15.293153</td>\n",
              "      <td>8.610086</td>\n",
              "      <td>6.259396</td>\n",
              "      <td>510.140768</td>\n",
              "      <td>26.171928</td>\n",
              "      <td>30.681973</td>\n",
              "      <td>31.925653</td>\n",
              "      <td>19.789531</td>\n",
              "      <td>10.072434</td>\n",
              "      <td>5.056717</td>\n",
              "      <td>6.218396</td>\n",
              "      <td>15.757824</td>\n",
              "      <td>24.449911</td>\n",
              "      <td>26.595599</td>\n",
              "      <td>27.210605</td>\n",
              "      <td>37.556376</td>\n",
              "      <td>37.050704</td>\n",
              "      <td>...</td>\n",
              "      <td>54.602613</td>\n",
              "      <td>40.613027</td>\n",
              "      <td>43.363788</td>\n",
              "      <td>12.280185</td>\n",
              "      <td>5.796247</td>\n",
              "      <td>3.438452</td>\n",
              "      <td>523.980745</td>\n",
              "      <td>5.422930</td>\n",
              "      <td>4.224384</td>\n",
              "      <td>11.828274</td>\n",
              "      <td>18.331860</td>\n",
              "      <td>15.089891</td>\n",
              "      <td>21.731015</td>\n",
              "      <td>18.685529</td>\n",
              "      <td>7.014441</td>\n",
              "      <td>155.241183</td>\n",
              "      <td>45.466156</td>\n",
              "      <td>71.185775</td>\n",
              "      <td>65.802142</td>\n",
              "      <td>56.272718</td>\n",
              "      <td>24.580018</td>\n",
              "      <td>1.689753</td>\n",
              "      <td>1.414677</td>\n",
              "      <td>1000</td>\n",
              "      <td>102.538696</td>\n",
              "      <td>82.960331</td>\n",
              "      <td>74.828305</td>\n",
              "      <td>79.133495</td>\n",
              "      <td>66.081252</td>\n",
              "      <td>78.245122</td>\n",
              "      <td>63.996993</td>\n",
              "      <td>47.322923</td>\n",
              "      <td>42.505211</td>\n",
              "      <td>70.420610</td>\n",
              "      <td>90.033143</td>\n",
              "      <td>98.677692</td>\n",
              "      <td>54.703249</td>\n",
              "      <td>20.125056</td>\n",
              "      <td>11.890525</td>\n",
              "      <td>16.537397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>94</td>\n",
              "      <td>251724</td>\n",
              "      <td>505.585483</td>\n",
              "      <td>32.054949</td>\n",
              "      <td>31.757004</td>\n",
              "      <td>28.102207</td>\n",
              "      <td>18.651380</td>\n",
              "      <td>12.080692</td>\n",
              "      <td>7.035483</td>\n",
              "      <td>7.686991</td>\n",
              "      <td>25.790151</td>\n",
              "      <td>42.129475</td>\n",
              "      <td>35.824951</td>\n",
              "      <td>32.058922</td>\n",
              "      <td>27.677138</td>\n",
              "      <td>33.842621</td>\n",
              "      <td>38.176733</td>\n",
              "      <td>32.722347</td>\n",
              "      <td>12.493842</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>11.504664</td>\n",
              "      <td>15.914255</td>\n",
              "      <td>16.394940</td>\n",
              "      <td>13.196994</td>\n",
              "      <td>8.648361</td>\n",
              "      <td>5.446441</td>\n",
              "      <td>494.414517</td>\n",
              "      <td>33.123580</td>\n",
              "      <td>28.082344</td>\n",
              "      <td>30.171934</td>\n",
              "      <td>16.863708</td>\n",
              "      <td>9.280005</td>\n",
              "      <td>5.390825</td>\n",
              "      <td>5.609318</td>\n",
              "      <td>19.453846</td>\n",
              "      <td>35.614403</td>\n",
              "      <td>32.082757</td>\n",
              "      <td>28.809331</td>\n",
              "      <td>27.911522</td>\n",
              "      <td>32.690566</td>\n",
              "      <td>...</td>\n",
              "      <td>88.227492</td>\n",
              "      <td>44.076261</td>\n",
              "      <td>87.939148</td>\n",
              "      <td>44.404973</td>\n",
              "      <td>9.671057</td>\n",
              "      <td>7.283569</td>\n",
              "      <td>502.912274</td>\n",
              "      <td>4.509700</td>\n",
              "      <td>0.980370</td>\n",
              "      <td>3.552398</td>\n",
              "      <td>5.986021</td>\n",
              "      <td>7.398907</td>\n",
              "      <td>9.740260</td>\n",
              "      <td>10.605292</td>\n",
              "      <td>7.485410</td>\n",
              "      <td>141.242417</td>\n",
              "      <td>43.078591</td>\n",
              "      <td>84.479020</td>\n",
              "      <td>52.069156</td>\n",
              "      <td>89.836451</td>\n",
              "      <td>33.932320</td>\n",
              "      <td>4.129086</td>\n",
              "      <td>3.886877</td>\n",
              "      <td>1000</td>\n",
              "      <td>61.632139</td>\n",
              "      <td>46.526521</td>\n",
              "      <td>48.437595</td>\n",
              "      <td>54.221644</td>\n",
              "      <td>51.680322</td>\n",
              "      <td>60.066684</td>\n",
              "      <td>54.790900</td>\n",
              "      <td>48.681562</td>\n",
              "      <td>43.873381</td>\n",
              "      <td>84.717507</td>\n",
              "      <td>112.204444</td>\n",
              "      <td>127.137252</td>\n",
              "      <td>83.019904</td>\n",
              "      <td>43.731067</td>\n",
              "      <td>38.851729</td>\n",
              "      <td>40.427349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>37382</td>\n",
              "      <td>495.586111</td>\n",
              "      <td>25.413301</td>\n",
              "      <td>29.318924</td>\n",
              "      <td>26.162324</td>\n",
              "      <td>19.260607</td>\n",
              "      <td>12.893906</td>\n",
              "      <td>6.580707</td>\n",
              "      <td>7.062222</td>\n",
              "      <td>17.334546</td>\n",
              "      <td>32.930287</td>\n",
              "      <td>28.302392</td>\n",
              "      <td>28.569900</td>\n",
              "      <td>26.804344</td>\n",
              "      <td>30.549462</td>\n",
              "      <td>36.595153</td>\n",
              "      <td>42.373335</td>\n",
              "      <td>16.398267</td>\n",
              "      <td>22.871970</td>\n",
              "      <td>17.174041</td>\n",
              "      <td>15.221229</td>\n",
              "      <td>23.433738</td>\n",
              "      <td>14.391953</td>\n",
              "      <td>7.383233</td>\n",
              "      <td>8.560270</td>\n",
              "      <td>504.413889</td>\n",
              "      <td>26.563587</td>\n",
              "      <td>30.255203</td>\n",
              "      <td>24.798031</td>\n",
              "      <td>16.237761</td>\n",
              "      <td>11.101600</td>\n",
              "      <td>4.788401</td>\n",
              "      <td>5.189663</td>\n",
              "      <td>17.842812</td>\n",
              "      <td>30.014445</td>\n",
              "      <td>27.767375</td>\n",
              "      <td>30.763469</td>\n",
              "      <td>25.199294</td>\n",
              "      <td>29.613183</td>\n",
              "      <td>...</td>\n",
              "      <td>102.957039</td>\n",
              "      <td>36.711921</td>\n",
              "      <td>70.039055</td>\n",
              "      <td>33.587502</td>\n",
              "      <td>5.021387</td>\n",
              "      <td>5.244560</td>\n",
              "      <td>511.177236</td>\n",
              "      <td>2.045750</td>\n",
              "      <td>3.236005</td>\n",
              "      <td>1.525014</td>\n",
              "      <td>7.476288</td>\n",
              "      <td>4.314674</td>\n",
              "      <td>8.554956</td>\n",
              "      <td>13.204389</td>\n",
              "      <td>7.773852</td>\n",
              "      <td>130.890831</td>\n",
              "      <td>53.784638</td>\n",
              "      <td>99.311884</td>\n",
              "      <td>57.095034</td>\n",
              "      <td>76.027525</td>\n",
              "      <td>38.422912</td>\n",
              "      <td>4.351869</td>\n",
              "      <td>3.161614</td>\n",
              "      <td>1000</td>\n",
              "      <td>51.125525</td>\n",
              "      <td>58.438255</td>\n",
              "      <td>68.930434</td>\n",
              "      <td>74.717029</td>\n",
              "      <td>63.970495</td>\n",
              "      <td>59.710034</td>\n",
              "      <td>58.883378</td>\n",
              "      <td>51.761414</td>\n",
              "      <td>47.310187</td>\n",
              "      <td>81.902582</td>\n",
              "      <td>93.793717</td>\n",
              "      <td>130.103014</td>\n",
              "      <td>71.982704</td>\n",
              "      <td>36.118530</td>\n",
              "      <td>31.603714</td>\n",
              "      <td>19.648989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>4</td>\n",
              "      <td>38106</td>\n",
              "      <td>482.312497</td>\n",
              "      <td>26.242586</td>\n",
              "      <td>21.072797</td>\n",
              "      <td>27.213562</td>\n",
              "      <td>15.588096</td>\n",
              "      <td>24.405605</td>\n",
              "      <td>19.367029</td>\n",
              "      <td>22.673595</td>\n",
              "      <td>30.100247</td>\n",
              "      <td>33.144387</td>\n",
              "      <td>29.575395</td>\n",
              "      <td>27.475988</td>\n",
              "      <td>24.274392</td>\n",
              "      <td>26.505012</td>\n",
              "      <td>29.785336</td>\n",
              "      <td>31.596074</td>\n",
              "      <td>9.893455</td>\n",
              "      <td>12.990080</td>\n",
              "      <td>11.625466</td>\n",
              "      <td>16.978953</td>\n",
              "      <td>15.430641</td>\n",
              "      <td>12.176560</td>\n",
              "      <td>7.872776</td>\n",
              "      <td>6.324463</td>\n",
              "      <td>517.687503</td>\n",
              "      <td>25.376581</td>\n",
              "      <td>25.035427</td>\n",
              "      <td>24.064452</td>\n",
              "      <td>14.328452</td>\n",
              "      <td>37.080775</td>\n",
              "      <td>24.248150</td>\n",
              "      <td>22.673595</td>\n",
              "      <td>36.450953</td>\n",
              "      <td>28.368236</td>\n",
              "      <td>25.953918</td>\n",
              "      <td>18.763449</td>\n",
              "      <td>30.808797</td>\n",
              "      <td>28.158295</td>\n",
              "      <td>...</td>\n",
              "      <td>75.763500</td>\n",
              "      <td>15.378600</td>\n",
              "      <td>79.282332</td>\n",
              "      <td>34.971111</td>\n",
              "      <td>6.820453</td>\n",
              "      <td>11.555671</td>\n",
              "      <td>510.578218</td>\n",
              "      <td>3.822929</td>\n",
              "      <td>2.259003</td>\n",
              "      <td>2.954081</td>\n",
              "      <td>19.505626</td>\n",
              "      <td>9.774534</td>\n",
              "      <td>9.079456</td>\n",
              "      <td>7.515531</td>\n",
              "      <td>4.778661</td>\n",
              "      <td>150.831922</td>\n",
              "      <td>42.269430</td>\n",
              "      <td>87.883922</td>\n",
              "      <td>27.238368</td>\n",
              "      <td>82.192971</td>\n",
              "      <td>43.703028</td>\n",
              "      <td>7.558973</td>\n",
              "      <td>9.209783</td>\n",
              "      <td>1000</td>\n",
              "      <td>155.386275</td>\n",
              "      <td>93.366590</td>\n",
              "      <td>63.233113</td>\n",
              "      <td>53.121208</td>\n",
              "      <td>55.750303</td>\n",
              "      <td>61.210732</td>\n",
              "      <td>52.986383</td>\n",
              "      <td>40.582446</td>\n",
              "      <td>44.559795</td>\n",
              "      <td>72.603479</td>\n",
              "      <td>90.467844</td>\n",
              "      <td>97.478765</td>\n",
              "      <td>57.165970</td>\n",
              "      <td>27.706620</td>\n",
              "      <td>19.347445</td>\n",
              "      <td>15.033032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728</th>\n",
              "      <td>3</td>\n",
              "      <td>32531</td>\n",
              "      <td>511.665796</td>\n",
              "      <td>31.908026</td>\n",
              "      <td>33.875380</td>\n",
              "      <td>28.618856</td>\n",
              "      <td>23.085672</td>\n",
              "      <td>14.294058</td>\n",
              "      <td>6.086502</td>\n",
              "      <td>4.949125</td>\n",
              "      <td>20.411300</td>\n",
              "      <td>40.730380</td>\n",
              "      <td>38.978205</td>\n",
              "      <td>32.000246</td>\n",
              "      <td>34.244259</td>\n",
              "      <td>37.010851</td>\n",
              "      <td>45.064708</td>\n",
              "      <td>46.540223</td>\n",
              "      <td>14.294058</td>\n",
              "      <td>13.894439</td>\n",
              "      <td>10.420829</td>\n",
              "      <td>11.711906</td>\n",
              "      <td>11.004888</td>\n",
              "      <td>7.131659</td>\n",
              "      <td>3.350650</td>\n",
              "      <td>2.059574</td>\n",
              "      <td>488.334204</td>\n",
              "      <td>29.295134</td>\n",
              "      <td>31.201008</td>\n",
              "      <td>31.293228</td>\n",
              "      <td>16.415112</td>\n",
              "      <td>11.865605</td>\n",
              "      <td>9.775291</td>\n",
              "      <td>4.457287</td>\n",
              "      <td>17.921367</td>\n",
              "      <td>34.859058</td>\n",
              "      <td>37.103071</td>\n",
              "      <td>38.271187</td>\n",
              "      <td>29.295134</td>\n",
              "      <td>34.336479</td>\n",
              "      <td>...</td>\n",
              "      <td>103.462212</td>\n",
              "      <td>32.376847</td>\n",
              "      <td>116.664421</td>\n",
              "      <td>49.306210</td>\n",
              "      <td>17.647851</td>\n",
              "      <td>8.397324</td>\n",
              "      <td>490.996453</td>\n",
              "      <td>3.367911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.706408</td>\n",
              "      <td>4.849791</td>\n",
              "      <td>3.098478</td>\n",
              "      <td>4.535453</td>\n",
              "      <td>5.074319</td>\n",
              "      <td>2.469801</td>\n",
              "      <td>95.379227</td>\n",
              "      <td>40.055683</td>\n",
              "      <td>97.085635</td>\n",
              "      <td>40.145494</td>\n",
              "      <td>128.294939</td>\n",
              "      <td>52.135255</td>\n",
              "      <td>9.160717</td>\n",
              "      <td>3.637343</td>\n",
              "      <td>1000</td>\n",
              "      <td>32.194156</td>\n",
              "      <td>24.269440</td>\n",
              "      <td>15.023939</td>\n",
              "      <td>25.672775</td>\n",
              "      <td>25.672775</td>\n",
              "      <td>30.460624</td>\n",
              "      <td>23.196302</td>\n",
              "      <td>32.359254</td>\n",
              "      <td>22.123163</td>\n",
              "      <td>71.570084</td>\n",
              "      <td>116.807000</td>\n",
              "      <td>181.442959</td>\n",
              "      <td>131.996038</td>\n",
              "      <td>106.653459</td>\n",
              "      <td>84.530296</td>\n",
              "      <td>76.027737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>3</td>\n",
              "      <td>38761</td>\n",
              "      <td>493.511519</td>\n",
              "      <td>32.558500</td>\n",
              "      <td>38.260107</td>\n",
              "      <td>29.075617</td>\n",
              "      <td>23.735198</td>\n",
              "      <td>12.770568</td>\n",
              "      <td>8.694306</td>\n",
              "      <td>8.900699</td>\n",
              "      <td>14.834499</td>\n",
              "      <td>29.978587</td>\n",
              "      <td>27.785661</td>\n",
              "      <td>25.902324</td>\n",
              "      <td>29.333609</td>\n",
              "      <td>29.591600</td>\n",
              "      <td>34.158045</td>\n",
              "      <td>36.737958</td>\n",
              "      <td>10.422848</td>\n",
              "      <td>22.006656</td>\n",
              "      <td>9.726271</td>\n",
              "      <td>17.956193</td>\n",
              "      <td>20.974691</td>\n",
              "      <td>12.177188</td>\n",
              "      <td>12.280385</td>\n",
              "      <td>5.650009</td>\n",
              "      <td>506.488481</td>\n",
              "      <td>32.584299</td>\n",
              "      <td>33.384072</td>\n",
              "      <td>32.971286</td>\n",
              "      <td>21.826062</td>\n",
              "      <td>12.693171</td>\n",
              "      <td>5.469415</td>\n",
              "      <td>3.044297</td>\n",
              "      <td>21.929259</td>\n",
              "      <td>28.637032</td>\n",
              "      <td>26.960089</td>\n",
              "      <td>31.681329</td>\n",
              "      <td>25.257346</td>\n",
              "      <td>30.958954</td>\n",
              "      <td>...</td>\n",
              "      <td>96.504794</td>\n",
              "      <td>38.199814</td>\n",
              "      <td>68.434890</td>\n",
              "      <td>25.595422</td>\n",
              "      <td>6.882153</td>\n",
              "      <td>1.121250</td>\n",
              "      <td>513.416332</td>\n",
              "      <td>3.131766</td>\n",
              "      <td>4.639654</td>\n",
              "      <td>5.915558</td>\n",
              "      <td>9.820600</td>\n",
              "      <td>13.107021</td>\n",
              "      <td>15.658831</td>\n",
              "      <td>6.302196</td>\n",
              "      <td>7.268791</td>\n",
              "      <td>177.969378</td>\n",
              "      <td>38.934426</td>\n",
              "      <td>80.884627</td>\n",
              "      <td>41.950201</td>\n",
              "      <td>78.912775</td>\n",
              "      <td>22.888958</td>\n",
              "      <td>4.369007</td>\n",
              "      <td>1.662543</td>\n",
              "      <td>1000</td>\n",
              "      <td>51.582409</td>\n",
              "      <td>59.460200</td>\n",
              "      <td>59.528703</td>\n",
              "      <td>60.282230</td>\n",
              "      <td>47.814769</td>\n",
              "      <td>65.693931</td>\n",
              "      <td>45.622688</td>\n",
              "      <td>46.033703</td>\n",
              "      <td>41.238526</td>\n",
              "      <td>85.628168</td>\n",
              "      <td>93.094945</td>\n",
              "      <td>129.949308</td>\n",
              "      <td>85.833676</td>\n",
              "      <td>49.390327</td>\n",
              "      <td>47.814769</td>\n",
              "      <td>31.031648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>1</td>\n",
              "      <td>28359</td>\n",
              "      <td>496.773511</td>\n",
              "      <td>36.954759</td>\n",
              "      <td>42.843542</td>\n",
              "      <td>32.793822</td>\n",
              "      <td>22.179908</td>\n",
              "      <td>13.681724</td>\n",
              "      <td>5.042491</td>\n",
              "      <td>5.148277</td>\n",
              "      <td>22.250432</td>\n",
              "      <td>31.594908</td>\n",
              "      <td>33.005395</td>\n",
              "      <td>28.068691</td>\n",
              "      <td>29.161818</td>\n",
              "      <td>33.463803</td>\n",
              "      <td>34.839028</td>\n",
              "      <td>26.940301</td>\n",
              "      <td>12.870694</td>\n",
              "      <td>17.842660</td>\n",
              "      <td>9.309214</td>\n",
              "      <td>13.716986</td>\n",
              "      <td>13.999083</td>\n",
              "      <td>10.190768</td>\n",
              "      <td>12.200712</td>\n",
              "      <td>8.674495</td>\n",
              "      <td>503.226489</td>\n",
              "      <td>38.153673</td>\n",
              "      <td>34.874290</td>\n",
              "      <td>39.246800</td>\n",
              "      <td>21.227829</td>\n",
              "      <td>12.482810</td>\n",
              "      <td>2.433090</td>\n",
              "      <td>5.113015</td>\n",
              "      <td>23.131986</td>\n",
              "      <td>33.569590</td>\n",
              "      <td>27.927642</td>\n",
              "      <td>29.020769</td>\n",
              "      <td>27.187136</td>\n",
              "      <td>31.453859</td>\n",
              "      <td>...</td>\n",
              "      <td>77.391734</td>\n",
              "      <td>32.932653</td>\n",
              "      <td>39.903398</td>\n",
              "      <td>18.991163</td>\n",
              "      <td>5.269224</td>\n",
              "      <td>2.469949</td>\n",
              "      <td>508.315495</td>\n",
              "      <td>8.178275</td>\n",
              "      <td>10.812888</td>\n",
              "      <td>13.831714</td>\n",
              "      <td>17.893408</td>\n",
              "      <td>12.404633</td>\n",
              "      <td>11.251990</td>\n",
              "      <td>9.276031</td>\n",
              "      <td>5.818102</td>\n",
              "      <td>150.337560</td>\n",
              "      <td>47.368132</td>\n",
              "      <td>92.595642</td>\n",
              "      <td>46.215489</td>\n",
              "      <td>48.795214</td>\n",
              "      <td>27.059663</td>\n",
              "      <td>4.775235</td>\n",
              "      <td>1.701520</td>\n",
              "      <td>1000</td>\n",
              "      <td>61.990212</td>\n",
              "      <td>44.909318</td>\n",
              "      <td>38.479992</td>\n",
              "      <td>85.788312</td>\n",
              "      <td>54.889166</td>\n",
              "      <td>74.560983</td>\n",
              "      <td>45.101238</td>\n",
              "      <td>57.863929</td>\n",
              "      <td>57.000288</td>\n",
              "      <td>83.485270</td>\n",
              "      <td>99.222723</td>\n",
              "      <td>162.364456</td>\n",
              "      <td>61.126571</td>\n",
              "      <td>29.939545</td>\n",
              "      <td>24.085980</td>\n",
              "      <td>19.192016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>507</td>\n",
              "      <td>5861000</td>\n",
              "      <td>484.835011</td>\n",
              "      <td>29.037878</td>\n",
              "      <td>29.160212</td>\n",
              "      <td>29.654325</td>\n",
              "      <td>18.743218</td>\n",
              "      <td>12.112609</td>\n",
              "      <td>6.963999</td>\n",
              "      <td>6.359666</td>\n",
              "      <td>19.319911</td>\n",
              "      <td>33.732298</td>\n",
              "      <td>32.182051</td>\n",
              "      <td>32.408463</td>\n",
              "      <td>34.287835</td>\n",
              "      <td>36.444805</td>\n",
              "      <td>35.989763</td>\n",
              "      <td>30.931752</td>\n",
              "      <td>11.007507</td>\n",
              "      <td>15.126088</td>\n",
              "      <td>9.101007</td>\n",
              "      <td>12.728033</td>\n",
              "      <td>16.913667</td>\n",
              "      <td>12.901723</td>\n",
              "      <td>10.217710</td>\n",
              "      <td>9.510493</td>\n",
              "      <td>515.164989</td>\n",
              "      <td>27.951373</td>\n",
              "      <td>27.868794</td>\n",
              "      <td>28.546835</td>\n",
              "      <td>17.895581</td>\n",
              "      <td>11.378604</td>\n",
              "      <td>6.441051</td>\n",
              "      <td>6.377239</td>\n",
              "      <td>19.027128</td>\n",
              "      <td>33.299266</td>\n",
              "      <td>32.904624</td>\n",
              "      <td>32.872547</td>\n",
              "      <td>35.759597</td>\n",
              "      <td>37.667463</td>\n",
              "      <td>...</td>\n",
              "      <td>63.302946</td>\n",
              "      <td>38.226452</td>\n",
              "      <td>90.190307</td>\n",
              "      <td>31.346224</td>\n",
              "      <td>16.056372</td>\n",
              "      <td>6.785838</td>\n",
              "      <td>525.737462</td>\n",
              "      <td>10.536660</td>\n",
              "      <td>6.467972</td>\n",
              "      <td>14.639804</td>\n",
              "      <td>9.941207</td>\n",
              "      <td>8.376625</td>\n",
              "      <td>7.333248</td>\n",
              "      <td>8.594764</td>\n",
              "      <td>13.652235</td>\n",
              "      <td>144.403249</td>\n",
              "      <td>27.106471</td>\n",
              "      <td>70.235342</td>\n",
              "      <td>51.378108</td>\n",
              "      <td>98.917071</td>\n",
              "      <td>38.619296</td>\n",
              "      <td>10.364382</td>\n",
              "      <td>5.171029</td>\n",
              "      <td>1000</td>\n",
              "      <td>83.305081</td>\n",
              "      <td>58.197145</td>\n",
              "      <td>59.940148</td>\n",
              "      <td>60.373944</td>\n",
              "      <td>55.253385</td>\n",
              "      <td>52.666237</td>\n",
              "      <td>49.324831</td>\n",
              "      <td>48.406921</td>\n",
              "      <td>42.351354</td>\n",
              "      <td>77.122990</td>\n",
              "      <td>93.503211</td>\n",
              "      <td>107.249387</td>\n",
              "      <td>73.718077</td>\n",
              "      <td>40.966917</td>\n",
              "      <td>44.925312</td>\n",
              "      <td>52.695060</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>732 rows × 190 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     # Purchases  B01001001   B01001002  ...   B19001015  B19001016  B19001017\n",
              "0             22     206252  469.226965  ...   36.440765  23.446284  21.197485\n",
              "1              7      61399  486.538869  ...   33.000508  33.169741  24.792689\n",
              "2              3      73170  489.859232  ...   20.125056  11.890525  16.537397\n",
              "3             94     251724  505.585483  ...   43.731067  38.851729  40.427349\n",
              "4              0      37382  495.586111  ...   36.118530  31.603714  19.648989\n",
              "..           ...        ...         ...  ...         ...        ...        ...\n",
              "727            4      38106  482.312497  ...   27.706620  19.347445  15.033032\n",
              "728            3      32531  511.665796  ...  106.653459  84.530296  76.027737\n",
              "729            3      38761  493.511519  ...   49.390327  47.814769  31.031648\n",
              "730            1      28359  496.773511  ...   29.939545  24.085980  19.192016\n",
              "731          507    5861000  484.835011  ...   40.966917  44.925312  52.695060\n",
              "\n",
              "[732 rows x 190 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TU7cyX1dJ9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listofallpredictors = allvariablenames[8:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5fcjukvam2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load predictors into dataframe\n",
        "predictors = alldata[listofallpredictors]  \n",
        "#load target into dataframe\n",
        "target = alldata['# Purchases']   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uze-CPE6dQNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data into train and test sets, with 30% retained for test\n",
        "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eh0iNfWepDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LassoLarsCV\n",
        "from sklearn.datasets import make_regression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmwjHNaGf0hI",
        "colab_type": "code",
        "outputId": "a1036559-3c50-459f-aebf-6401f0d217df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X, y = make_regression(noise=4.0, random_state=0)\n",
        "#model = LassoLarsCV(precompute=False,cv=10).fit(X, y)\n",
        "#model.score(X, y)\n",
        "model = LassoLarsCV(precompute=False, cv=10)\n",
        "model.fit(pred_train, tar_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.365e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.008e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.144e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.642e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.644e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.626e-01, previous alpha=5.354e-01, with an active set of 13 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=7.201e-01, previous alpha=7.200e-01, with an active set of 12 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.546e-02, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.539e-02, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 116 iterations, alpha=3.652e-02, previous alpha=3.599e-02, with an active set of 99 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.523e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 68 iterations, alpha=6.435e-02, previous alpha=6.382e-02, with an active set of 61 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.147e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.014e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.010e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.693e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.099e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.948e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.137e-01, previous alpha=3.116e-01, with an active set of 27 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.644e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.822e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 9.125e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 37 iterations, alpha=2.062e-01, previous alpha=1.984e-01, with an active set of 34 regressors.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
            "  ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.186e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n",
              "            max_iter=500, max_n_alphas=1000, n_jobs=None, normalize=True,\n",
              "            positive=False, precompute=False, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghn7Gg4HgCw2",
        "colab_type": "code",
        "outputId": "c8644748-9997-4c8d-bb04-12822875feb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#build coefficent chart\n",
        "\n",
        "predictors_model=pd.DataFrame(listofallpredictors)\n",
        "predictors_model.columns = ['label']\n",
        "predictors_model['coeff'] = model.coef_\n",
        "\n",
        "for index, row in predictors_model.iterrows():\n",
        "    if row['coeff'] > 0:\n",
        "        print(row.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B01001014' 0.8558761066941788]\n",
            "['B01001036' 2.5053482381631653]\n",
            "['B01001037' 0.8892493223320962]\n",
            "['B01001038' 1.5316387928880384]\n",
            "['B02001005' 0.41252295298457853]\n",
            "['B13014026' 0.48004105312075906]\n",
            "['B13014027' 0.6978957445987839]\n",
            "['B13016001' 875149895.329212]\n",
            "['B19001017' 1.4834348068681533]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKOs_PDRhz8l",
        "colab_type": "text"
      },
      "source": [
        "Question 1 : In your own words, explain what the above lines of code are doing. Why am I doing it? Explain each line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p04oj8ONh-bh",
        "colab_type": "text"
      },
      "source": [
        "Question 2 : Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? Here's what I'd put for the example above. \"In areas where there are more females aged 30-34, we sell more Bobo Bars.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brQ09hVQiEHw",
        "colab_type": "text"
      },
      "source": [
        "Question 3 : If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojZI3xnh3-M",
        "colab_type": "code",
        "outputId": "5d82025a-4667-4e32-8b6d-f442ed55b5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# mean squared error \n",
        "from sklearn.metrics import mean_squared_error\n",
        "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
        "print ('training data MSE')\n",
        "print(train_error)\n",
        "\n",
        "test_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
        "print ('testing data MSE')\n",
        "print(test_error)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data MSE\n",
            "22025.491066757\n",
            "testing data MSE\n",
            "41549.54803776253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F3I6A7jiJH4",
        "colab_type": "text"
      },
      "source": [
        "Question 4 Are the training and text set mean squared errors similar? What does that mean practically? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUEqA3ZkiLDS",
        "colab_type": "code",
        "outputId": "38dd3611-dbc4-478a-c218-fa5599339b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#r squared\n",
        "rsquared_train=model.score(pred_train,tar_train)\n",
        "print ('training data R-square')\n",
        "print(rsquared_train)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data R-square\n",
            "0.2400221219784492\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJSlSUolYVOg",
        "colab_type": "code",
        "outputId": "39cef665-c826-41cd-dc93-8e96a629d188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rsquared_test = model.score(pred_test, tar_test)\n",
        "print ('testing data R-square')\n",
        "print(rsquared_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing data R-square\n",
            "0.1758628512005107\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsar2s5EYsE0",
        "colab_type": "text"
      },
      "source": [
        "The MSE for training data(22025.49) and testing data (41549.548) are significantly different. The training data MSE is fitting better than testing data becasue the MSE is a measure of the quality of an estimator it is always non-negative, and values closer to zero are better. \n",
        "Moreover, Compare the two R Squared values The R squared value for the training set is slightly higher, indicating that this model is slight more accurate at explaining the variability in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tktiGt0liTuu",
        "colab_type": "text"
      },
      "source": [
        "Question 5  If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?\n",
        "\n",
        "Finally, let's see what our y-intercept is, so we can interpret what our baseline sales number looks like, all things considered:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwLoj7VQiRYp",
        "colab_type": "code",
        "outputId": "54c2522f-6e81-4ea0-95f6-77979f71a346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"y interecept:\")\n",
        "print(model.intercept_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y interecept:\n",
            "22.19738813257551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxBaR1m4iWkq",
        "colab_type": "text"
      },
      "source": [
        "Question 6 : What is our baseline sales number? What does that mean, practically? Think back to what y-intercepts mean in regression models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM9ek6br3szu",
        "colab_type": "text"
      },
      "source": [
        "Our baseline sales number (y-intercept) is 2.75. This means that if all other variables are zero, we sell an average of 2.75 bars per customer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm7gZf2g3tQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}